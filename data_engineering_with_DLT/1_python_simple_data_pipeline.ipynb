{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edd635a1-d07a-4e64-9429-cf4628c2e8e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Simplify ETL with Delta Live Table\n",
    "\n",
    "DLT makes Data Engineering accessible for all. Just declare your transformations in SQL or Python, and DLT will handle the Data Engineering complexity for you.\n",
    "\n",
    "<img style=\"float:right\" src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/dlt-golden-demo-loan-1.png\" width=\"700\"/>\n",
    "\n",
    "**Accelerate ETL development** <br/>\n",
    "Enable analysts and data engineers to innovate rapidly with simple pipeline development and maintenance \n",
    "\n",
    "**Remove operational complexity** <br/>\n",
    "By automating complex administrative tasks and gaining broader visibility into pipeline operations\n",
    "\n",
    "**Trust your data** <br/>\n",
    "With built-in quality controls and quality monitoring to ensure accurate and useful BI, Data Science, and ML \n",
    "\n",
    "**Simplify batch and streaming** <br/>\n",
    "With self-optimization and auto-scaling data pipelines for batch or streaming processing \n",
    "\n",
    "## Our Delta Live Table pipeline\n",
    "\n",
    "We will be using one table of the provided AXPO data about clearing prices/volume of power data from France."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "760f6543-2353-444f-8338-58a3bfe00dca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "The CSV data that we utilize here is already landed on a volume and is ready to be ingested from there. In this simple example you will just find one CSV file on that volume (_/Volumes/dbw-databricks-dna-hackathon-databricks-stream/alexander_genser/data_volume/data/_). The CSV named 'dataset_3.csv' contains the clearing prices/volume of the data from France. Note that in a production scenario, new recores of data will arrive in batches or in a streaming fashion. Nevertheless, the technology to ingest and transform the data remains, [Autoloader](https://learn.microsoft.com/en-us/azure/databricks/ingestion/cloud-object-storage/auto-loader/) is the perfect tool for such use-cases as Auto Loader incrementally and efficiently processes new data files as they arrive in cloud storage.\n",
    "\n",
    "We use the keyword `INCREMENTAL` and `readstream()` to indicate we are incrementally loading data. Without `INCREMENTAL` or by just using `read()`  you will scan and ingest all the data available at once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd9e3a0-3399-4392-a7a3-48c49d7fcf25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import dlt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import col, weekofyear, year, expr, round, avg, to_date, concat, lit, sum, window, date_format\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# path to volume where data is landed (you can find the volume in Unity catalog)\n",
    "source = '/Volumes/dbw_databricks_dna_hackathon_databricks_stream/alexander_genser/data_volume/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "beb88e94-b75d-4503-b683-f613b17be841",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "# What is Databricks Auto Loader?\n",
    "\n",
    "<img src=\"https://github.com/QuentinAmbard/databricks-demo/raw/main/product_demos/autoloader/autoloader-edited-anim.gif\" style=\"float:right; margin-left: 10px\" />\n",
    "\n",
    "[Databricks Auto Loader](https://docs.databricks.com/ingestion/auto-loader/index.html) lets you scan a cloud storage folder (S3, ADLS, GS) and only ingest the new data that arrived since the previous run.\n",
    "\n",
    "This is called **incremental ingestion**.\n",
    "\n",
    "Auto Loader can be used in a near real-time stream or in a batch fashion, e.g., running every night to ingest daily data.\n",
    "\n",
    "Auto Loader provides a strong gaurantee when used with a Delta sink (the data will only be ingested once).\n",
    "\n",
    "## How Auto Loader simplifies data ingestion\n",
    "\n",
    "Ingesting data at scale from cloud storage can be really hard at scale. Auto Loader makes it easy, offering these benefits:\n",
    "\n",
    "\n",
    "* **Incremental** & **cost-efficient** ingestion (removes unnecessary listing or state handling)\n",
    "* **Simple** and **resilient** operation: no tuning or manual code required\n",
    "* Scalable to **billions of files**\n",
    "  * Using incremental listing (recommended, relies on filename order)\n",
    "  * Leveraging notification + message queue (when incremental listing can't be used)\n",
    "* **Schema inference** and **schema evolution** are handled out of the box for most formats (csv, json, avro, images...)\n",
    "\n",
    "<img width=\"1px\" src=\"https://ppxrzfxige.execute-api.us-west-2.amazonaws.com/v1/analytics?category=data-engineering&org_id=1444828305810485&notebook=%2F01-Auto-loader-schema-evolution-Ingestion&demo_name=auto-loader&event=VIEW&path=%2F_dbdemos%2Fdata-engineering%2Fauto-loader%2F01-Auto-loader-schema-evolution-Ingestion&version=1\">\n",
    "\n",
    "<img style=\"float: center; padding-left: 10px\" src=\"https://github.com/genseral/axpo_dna_summit_2024/blob/main/figures/Power_data_simple_DLT.drawio.png?raw=true\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5508a9d6-6fe6-48c4-aab2-5b44017acf8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name = f\"bronze_simple_power\",\n",
    "    comment = \"Raw data power data from france\",\n",
    ")\n",
    "@dlt.expect_or_drop(\"vaild_volume\", F.col(\"clearing_volume\") != 'NULL')\n",
    "def bronze_simple_power():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "        .load(f\"{source}dataset_3\")\n",
    "        .select(\"*\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2abd9a19-76dd-4161-b1bc-5d78bc225ef2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Silver layer: joining tables while ensuring data quality\n",
    "\n",
    "<img style=\"float: right; padding-left: 10px\" src=\"https://github.com/genseral/axpo_dna_summit_2024/blob/main/figures/Power_data_simple_DLT_silver.drawio.png?raw=true\" width=\"600\"/>\n",
    "\n",
    "Once the bronze layer is defined, we'll create the sliver layers by Joining data. Note that bronze tables are referenced using the `LIVE` spacename. \n",
    "\n",
    "To consume only increment from the Bronze layer like `raw_txs`, we'll be using the `stream` keyworkd: `stream(LIVE.raw_txs)`\n",
    "\n",
    "Note that we don't have to worry about compactions, DLT handles that for us.\n",
    "\n",
    "#### Expectations\n",
    "By defining expectations (`CONSTRAINT <name> EXPECT <condition>`), you can enforce and track your data quality. See the [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-expectations.html) for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44e45cf3-c99f-48b0-9fd5-9e776a5c4489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name = f\"silver_simple_power\",\n",
    "    comment=\"Cleaned raw power data from france\"\n",
    ")\n",
    "def silver_simple_power():\n",
    "    raw_power_fra =  dlt.read(\"bronze_simple_power\")\n",
    "\n",
    "    raw_power_fra = raw_power_fra.filter(F.col(\"DeliveryStartDate\") <= F.col(\"DeliveryEndDate\"))\n",
    "    raw_power_fra = raw_power_fra.filter(F.col(\"DeliveryStartDate\") == F.col(\"AdjustedDeliveryStartDate\"))\n",
    "    raw_power_fra = raw_power_fra.filter(F.col(\"DeliveryEndDate\") == F.col(\"AdjustedDeliveryEndDate\"))\n",
    "    \n",
    "    return (\n",
    "        raw_power_fra.select(\n",
    "                col(\"TimeSeries_FID\"),\n",
    "                col(\"PublicationDateIndex_FID\"),\n",
    "                col(\"PublicationDate\"),\n",
    "                col(\"QuoteDateIndex_FID\"),\n",
    "                col(\"QuoteTime\"),\n",
    "                col(\"DeliveryGridPointName\"),\n",
    "                col(\"DeliveryStartDate\"),\n",
    "                col(\"DeliveryEndDate\"),\n",
    "                col(\"DeliveryRelativeBucketName\"),\n",
    "                col(\"DeliveryRelativeBucketNumber\"),\n",
    "                col(\"clearing_price\"),\n",
    "                col(\"clearing_volume\"),\n",
    "                col(\"Point_ID\")\n",
    "            )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5240abea-0074-42be-a192-bb9544700701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Gold layer\n",
    "\n",
    "<img style=\"float: right; padding-left: 10px\" src=\"https://github.com/genseral/axpo_dna_summit_2024/blob/main/figures/Power_data_simple_DLT_gold.drawio.png?raw=true\" width=\"600\"/>\n",
    "\n",
    "Our last step is to materialize the Gold Layer.\n",
    "\n",
    "Because these tables will be requested at scale using a SQL Endpoint, we'll add Zorder at the table level to ensure faster queries using `pipelines.autoOptimize.zOrderCols`, and DLT will handle the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6f0774e5-219e-48ab-9ac7-53b7ea3d7eb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "    name = f\"gold_simple_power\",\n",
    "    comment=\"Aggregated Power data from france\"\n",
    ")\n",
    "def gold_simple_power():\n",
    "    return (\n",
    "        dlt.read(\"silver_simple_power\")\n",
    "            .withColumn(\"DeliveryDate\", F.to_date(\"DeliveryStartDate\"))\n",
    "            .select(\"DeliveryDate\", \"clearing_price\", \"clearing_volume\")\n",
    "            .groupBy(\"DeliveryDate\")\n",
    "            .agg(\n",
    "                F.round(F.avg(\"clearing_price\"), 0).alias(\"avg_clearing_price\"),\n",
    "                F.sum(\"clearing_volume\").alias(\"total_clearing_volume\")\n",
    "            )\n",
    "            .orderBy(\"DeliveryDate\")\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "1_python_simple_data_pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}